{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import data_loader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "N_EPOCHS = 1\n",
    "LR = 0.05\n",
    "NUM_CLASSES = 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the image data\n",
    "\n",
    "https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader_train, image_loader_validation = data_loader.load_data(BATCH_SIZE)\n",
    "\n",
    "dataiter = iter(image_loader_train)\n",
    "#print(len(image_loader_train))\n",
    "test_mini_batch = next(dataiter)\n",
    "\n",
    "# for batch_number, sample in enumerate(image_loader_train):\n",
    "#     print(batch_number, sample[\"image\"].shape, sample[\"target_labels\"].shape)\n",
    "    \n",
    "#     if batch_number == 4:\n",
    "#         break\n",
    "\n",
    "#print(mini_batch[\"image\"].shape)\n",
    "#print(mini_batch[\"target_labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test batch:  torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet34(weights='DEFAULT')\n",
    "\n",
    "print(\"test batch: \", test_mini_batch[\"image\"].shape, test_mini_batch[\"target_labels\"].shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = resnet(test_mini_batch[\"image\"])\n",
    "    \n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement early stopping for regularization\n",
    "(mun assignment kakkosesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(\"Early stopping due to improvement halt\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet34(weights='DEFAULT')\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.resnet.fc.in_features, out_features=NUM_CLASSES)\n",
    "            )\n",
    "        self.model = self.resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilabelClassifier()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=LR)\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_prediction(outputs, threshold=0.5):\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "        prediction = [np.float32(1.0) if i>0.5 else np.float32(0.0) for i in output]\n",
    "        predictions.append(prediction)\n",
    "    return torch.tensor(predictions).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if all of the 14 labels are predicted correctly for one image (return True/False)\n",
    "def prediction_fully_correct(prediction, target_label):\n",
    "    return torch.equal(prediction, target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks how many of the labels are predicted correctly for one image (returns int between 0 and 14)\n",
    "def correct_labels_in_prediction(prediction, target_labels):\n",
    "    return (prediction == target_labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test batch:  torch.Size([2, 3, 224, 224]) torch.Size([2, 14])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0.0014, 0.0055, 0.0092, 0.0595, 0.0224, 0.1355, 0.0244, 0.1279, 0.0462,\n",
      "         0.2953, 0.1520, 0.0076, 0.0042, 0.0330],\n",
      "        [0.0047, 0.0213, 0.0132, 0.0581, 0.0377, 0.1930, 0.0382, 0.1787, 0.0091,\n",
      "         0.3825, 0.2138, 0.0032, 0.0086, 0.0264]], device='cuda:0')\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "True\n",
      "True\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(\"test batch: \", test_mini_batch[\"image\"].shape, test_mini_batch[\"target_labels\"].shape)\n",
    "test_target = test_mini_batch[\"target_labels\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_mini_batch[\"image\"].to(device))\n",
    "    \n",
    "print(test_mini_batch[\"target_labels\"])\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "prediction = output_to_prediction(outputs)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "correct_prediction = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32)\n",
    "\n",
    "print(correct_labels_in_prediction(prediction[0], test_target[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 1/8000: Loss: 0.3292 | Train fully correct: 0.000% (0/2) - Train labels correct: 82.143% (23/28)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 2/8000: Loss: 0.3094 | Train fully correct: 0.000% (0/4) - Train labels correct: 83.929% (47/56)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 3/8000: Loss: 0.2928 | Train fully correct: 16.667% (1/6) - Train labels correct: 88.095% (74/84)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 4/8000: Loss: 0.2653 | Train fully correct: 25.000% (2/8) - Train labels correct: 90.179% (101/112)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 5/8000: Loss: 0.2698 | Train fully correct: 20.000% (2/10) - Train labels correct: 89.286% (125/140)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 6/8000: Loss: 0.2546 | Train fully correct: 25.000% (3/12) - Train labels correct: 89.881% (151/168)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 7/8000: Loss: 0.2294 | Train fully correct: 35.714% (5/14) - Train labels correct: 91.327% (179/196)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 8/8000: Loss: 0.2112 | Train fully correct: 43.750% (7/16) - Train labels correct: 92.411% (207/224)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 9/8000: Loss: 0.2235 | Train fully correct: 38.889% (7/18) - Train labels correct: 91.667% (231/252)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 10/8000: Loss: 0.2164 | Train fully correct: 40.000% (8/20) - Train labels correct: 91.786% (257/280)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 11/8000: Loss: 0.2108 | Train fully correct: 40.909% (9/22) - Train labels correct: 91.883% (283/308)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 12/8000: Loss: 0.1988 | Train fully correct: 45.833% (11/24) - Train labels correct: 92.560% (311/336)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 13/8000: Loss: 0.2024 | Train fully correct: 42.308% (11/26) - Train labels correct: 92.308% (336/364)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 14/8000: Loss: 0.2071 | Train fully correct: 42.857% (12/28) - Train labels correct: 91.837% (360/392)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 15/8000: Loss: 0.2113 | Train fully correct: 43.333% (13/30) - Train labels correct: 91.667% (385/420)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 16/8000: Loss: 0.2123 | Train fully correct: 43.750% (14/32) - Train labels correct: 91.964% (412/448)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 17/8000: Loss: 0.2188 | Train fully correct: 41.176% (14/34) - Train labels correct: 91.387% (435/476)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 18/8000: Loss: 0.2175 | Train fully correct: 38.889% (14/36) - Train labels correct: 91.468% (461/504)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 19/8000: Loss: 0.2106 | Train fully correct: 42.105% (16/38) - Train labels correct: 91.917% (489/532)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 20/8000: Loss: 0.2141 | Train fully correct: 40.000% (16/40) - Train labels correct: 91.607% (513/560)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 21/8000: Loss: 0.2124 | Train fully correct: 40.476% (17/42) - Train labels correct: 91.667% (539/588)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 22/8000: Loss: 0.2154 | Train fully correct: 38.636% (17/44) - Train labels correct: 91.396% (563/616)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 23/8000: Loss: 0.2179 | Train fully correct: 36.957% (17/46) - Train labels correct: 90.994% (586/644)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 24/8000: Loss: 0.2288 | Train fully correct: 35.417% (17/48) - Train labels correct: 90.923% (611/672)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 25/8000: Loss: 0.2277 | Train fully correct: 36.000% (18/50) - Train labels correct: 90.857% (636/700)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 26/8000: Loss: 0.2232 | Train fully correct: 36.538% (19/52) - Train labels correct: 91.071% (663/728)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 27/8000: Loss: 0.2200 | Train fully correct: 37.037% (20/54) - Train labels correct: 91.270% (690/756)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 28/8000: Loss: 0.2200 | Train fully correct: 35.714% (20/56) - Train labels correct: 91.199% (715/784)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 29/8000: Loss: 0.2158 | Train fully correct: 37.931% (22/58) - Train labels correct: 91.502% (743/812)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 30/8000: Loss: 0.2193 | Train fully correct: 36.667% (22/60) - Train labels correct: 91.190% (766/840)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 31/8000: Loss: 0.2170 | Train fully correct: 37.097% (23/62) - Train labels correct: 91.244% (792/868)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 32/8000: Loss: 0.2132 | Train fully correct: 39.062% (25/64) - Train labels correct: 91.518% (820/896)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 33/8000: Loss: 0.2159 | Train fully correct: 37.879% (25/66) - Train labels correct: 91.342% (844/924)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 34/8000: Loss: 0.2153 | Train fully correct: 38.235% (26/68) - Train labels correct: 91.282% (869/952)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 35/8000: Loss: 0.2149 | Train fully correct: 38.571% (27/70) - Train labels correct: 91.327% (895/980)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 36/8000: Loss: 0.2167 | Train fully correct: 37.500% (27/72) - Train labels correct: 91.171% (919/1008)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 37/8000: Loss: 0.2272 | Train fully correct: 36.486% (27/74) - Train labels correct: 90.927% (942/1036)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 38/8000: Loss: 0.2259 | Train fully correct: 36.842% (28/76) - Train labels correct: 90.883% (967/1064)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 39/8000: Loss: 0.2292 | Train fully correct: 35.897% (28/78) - Train labels correct: 90.934% (993/1092)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 40/8000: Loss: 0.2263 | Train fully correct: 37.500% (30/80) - Train labels correct: 91.161% (1021/1120)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 41/8000: Loss: 0.2258 | Train fully correct: 37.805% (31/82) - Train labels correct: 91.115% (1046/1148)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 42/8000: Loss: 0.2246 | Train fully correct: 38.095% (32/84) - Train labels correct: 91.156% (1072/1176)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 43/8000: Loss: 0.2287 | Train fully correct: 37.209% (32/86) - Train labels correct: 90.947% (1095/1204)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 44/8000: Loss: 0.2289 | Train fully correct: 37.500% (33/88) - Train labels correct: 90.909% (1120/1232)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 45/8000: Loss: 0.2289 | Train fully correct: 36.667% (33/90) - Train labels correct: 90.952% (1146/1260)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 46/8000: Loss: 0.2310 | Train fully correct: 35.870% (33/92) - Train labels correct: 90.839% (1170/1288)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 47/8000: Loss: 0.2337 | Train fully correct: 35.106% (33/94) - Train labels correct: 90.729% (1194/1316)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 48/8000: Loss: 0.2346 | Train fully correct: 34.375% (33/96) - Train labels correct: 90.551% (1217/1344)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 49/8000: Loss: 0.2321 | Train fully correct: 35.714% (35/98) - Train labels correct: 90.743% (1245/1372)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 50/8000: Loss: 0.2297 | Train fully correct: 36.000% (36/100) - Train labels correct: 90.857% (1272/1400)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 51/8000: Loss: 0.2269 | Train fully correct: 37.255% (38/102) - Train labels correct: 91.036% (1300/1428)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 52/8000: Loss: 0.2243 | Train fully correct: 38.462% (40/104) - Train labels correct: 91.209% (1328/1456)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 53/8000: Loss: 0.2220 | Train fully correct: 39.623% (42/106) - Train labels correct: 91.375% (1356/1484)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 54/8000: Loss: 0.2223 | Train fully correct: 39.815% (43/108) - Train labels correct: 91.468% (1383/1512)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 55/8000: Loss: 0.2222 | Train fully correct: 40.000% (44/110) - Train labels correct: 91.429% (1408/1540)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 56/8000: Loss: 0.2200 | Train fully correct: 41.071% (46/112) - Train labels correct: 91.582% (1436/1568)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 57/8000: Loss: 0.2192 | Train fully correct: 41.228% (47/114) - Train labels correct: 91.541% (1461/1596)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 58/8000: Loss: 0.2194 | Train fully correct: 41.379% (48/116) - Train labels correct: 91.502% (1486/1624)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 59/8000: Loss: 0.2201 | Train fully correct: 41.525% (49/118) - Train labels correct: 91.465% (1511/1652)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Training: Epoch 0 - Batch 60/8000: Loss: 0.2254 | Train fully correct: 40.833% (49/120) - Train labels correct: 91.369% (1535/1680)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 18\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     20\u001b[0m total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m target_labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m predictions \u001b[39m=\u001b[39m output_to_prediction(outputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopper = EarlyStopper(patience=3)\n",
    "model.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    total_correct_labels = 0\n",
    "    for batch_number, data in enumerate(image_loader_train):\n",
    "        images, target_labels = data['image'].to(device), data['target_labels'].to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        #print(\"outputs: \", outputs.is_cuda)\n",
    "        loss = loss_function(outputs, target_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        total += target_labels.size(0)\n",
    "        predictions = output_to_prediction(outputs)\n",
    "        #print(\"prediction: \", predictions.is_cuda)\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            if prediction_fully_correct(prediction, target_labels[i]):\n",
    "                train_correct += 1\n",
    "            total_correct_labels += correct_labels_in_prediction(prediction, target_labels[i])\n",
    "\n",
    "        print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Train fully correct: %.3f%% (%d/%d) - Train labels correct: %.3f%% (%d/%d)' % \n",
    "              (epoch, batch_number + 1, len(image_loader_train), train_loss / (batch_number + 1), \n",
    "               100. * train_correct / total, train_correct, total, 100. * total_correct_labels / (total*14), total_correct_labels, (total*14)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
